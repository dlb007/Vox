{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Exercise Eleven: PCA\n",
    "\n",
    "Drawing on our example from class and the discussion of PCA in Data-Sitters Club,\n",
    "\n",
    "- Import at least ten documents from files, using the OS module and any others relevant to process the text\n",
    "\n",
    "- Isolate a component (the example was nouns - try verbs or adjectives) using nltk and prepare appropriate sub-files for comparison on that axis\n",
    "\n",
    "- Load the documents and titles and run the contents through vectorize, using the provided boilerplate\n",
    "\n",
    "- Run a simple (2 word) vizualization comparing all texts\n",
    "\n",
    "- Run a full (PCA) vizualization comparing all texts using the provided PCA boilerplate. Note any interesting characteristics or outliers in a brief \n",
    " analysis\n",
    " \n",
    "\n",
    "Bonus Challenge: Depending on your interests, you might either try using an API to collect the texts rather than saving them to a directory, or you might use Bokeh to attempt an exportable visualization of some aspect of what you've collected.\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell 126).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage One: Import at least ten documents from files, using the OS module and any others relevant to process the text\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell 126).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our first demo is adapted from https://datasittersclub.github.io/site/dsc10.html\n",
    "# But be warned, it switches to R - we will be using Python throughout!\n",
    "# #NLTK is the NLP package we're using\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#OS is for navigating directories on the computer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "textdir = 'C:\\\\DesignDevSyllabus\\\\exercises\\eleven\\\\text\\\\'\n",
    "os.chdir(textdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Two: Isolate a component (the example was nouns - try verbs or adjectives) using nltk and prepare appropriate sub-files for comparison on that axis\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell 126)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each filename in the directory you listed...\n",
    "for filename in os.listdir(textdir):\n",
    "    #If the filename ends with .txt...\n",
    "    if filename.endswith('.txt'):\n",
    "        #Create an output name that adds '-nouns' to the filename\n",
    "        outname = filename.replace('.txt','-nouns.txt')\n",
    "        #Open the file\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            #Open the output file\n",
    "            with open(outname, 'w') as out:\n",
    "                #Read the text from the file\n",
    "                text = f.read()\n",
    "                #Split the text into a list of sentences\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                #For each sentence in the list of sentences...\n",
    "                for sentence in sentences:\n",
    "                    #For each word and each part-of-speech tag that you get\n",
    "                    #When NLTK tokenizes the sentence (splitting words from punctuation, etc.)\n",
    "                    for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):\n",
    "                        #If the part-of-speech is noun\n",
    "                        if (pos == 'NN' or pos == 'NNS'):\n",
    "                        #You can sub in other parts of speech, too\n",
    "                        #Just comment out the noun code, and uncomment one of these\n",
    "                        #Adverbs\n",
    "                        #if (pos == 'RB' or pos == 'RBR' or pos == 'RBS'):\n",
    "                        #Adjectives\n",
    "                        #if (pos == 'JJ' or pos == 'JJR' or pos == 'JJS'):\n",
    "                        #Verbs\n",
    "                        #if (pos == 'VB' or pos == 'VBD' or pos == 'VBG' or pos == 'VBN' or pos == 'VBP' or pos == 'VBZ'):\n",
    "                            #Write the word (which should be a noun) to the output file\n",
    "                            out.write(word)\n",
    "                            #Write a space so the words don't smush together\n",
    "                            out.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Three: Load the documents and titles and run the contents through vectorize, using the provided boilerplate\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell 126)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_directory(directory, max_length):\n",
    "    documents, titles = [], []\n",
    "    for filename in os.scandir(directory):\n",
    "        #change this to analyze a different component after changing the save above\n",
    "        if not filename.name.endswith('-nouns.txt'):\n",
    "            continue\n",
    "\n",
    "        with open(filename.path) as f:\n",
    "            contents = f.read()\n",
    "        lemmas = contents.lower().split()\n",
    "        start_idx, end_idx, segm_cnt = 0, max_length, 1\n",
    "\n",
    "        # extract slices from the text:\n",
    "        while end_idx < len(lemmas):\n",
    "            documents.append(' '.join(lemmas[start_idx:end_idx]))\n",
    "            title = filename.name.replace('-nouns.txt', '')\n",
    "            titles.append(f\"{title}-{segm_cnt}\")\n",
    "\n",
    "            start_idx += max_length\n",
    "            end_idx += max_length\n",
    "            segm_cnt += 1\n",
    "\n",
    "    return documents, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Four: Run a simple (2 word) vizualization comparing all texts\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell 126)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, titles = load_directory(textdir, 10000)\n",
    "import sklearn.feature_extraction.text as text\n",
    "print(titles[0])\n",
    "\n",
    "vectorizer = text.CountVectorizer(max_features=30, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "v_documents = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "print(v_documents.shape)\n",
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Five: Run a full (PCA) vizualization comparing all texts using the provided PCA boilerplate. Note any interesting characteristics or outliers in a brief analysis\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell 126)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "titles = np.array(titles)\n",
    "x = v_documents[:, words.index('cave')]\n",
    "y = v_documents[:, words.index('claws')]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for title in set(titles):\n",
    "    ax.scatter(x[titles==title], y[titles==title], label=title)\n",
    "ax.set(xlabel='cave', ylabel='dragon')\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=2)\n",
    "documents_proj = pca.fit_transform(v_documents)\n",
    "\n",
    "print(v_documents.shape)\n",
    "print(documents_proj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = documents_proj[:, 0], documents_proj[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(c1, c2, facecolors='none')\n",
    "\n",
    "for p1, p2, title in zip(c1, c2, titles):\n",
    "    ax.text(p1, p2, title[4], fontsize=12,\n",
    "            ha='center', va='center')\n",
    "\n",
    "ax.set(xlabel='PC1', ylabel='PC2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Stage: Depending on your interests, you might either try using an API to collect the texts rather than saving them to a directory, or you might use Bokeh to attempt an exportable visualization of some aspect of what you've collected.\n",
    "\n",
    "(Karsdorp, Kestemont, and Riddell 126).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "#file for output\n",
    "output_file(filename=\"amazon.html\", title=\"Amazon CSV Visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('amazon.csv')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.models.tools import HoverTool\n",
    "from bokeh.io import output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(100)\n",
    "source = ColumnDataSource(sample)\n",
    "\n",
    "p = figure()\n",
    "p.circle(x='Price', y='User Rating',\n",
    "         source=source,\n",
    "         size=10, color='green')\n",
    "\n",
    "p.title.text = 'Amazon Bestsellers'\n",
    "p.xaxis.axis_label = 'Price'\n",
    "p.yaxis.axis_label = 'User Rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hover = HoverTool()\n",
    "hover.tooltips=[\n",
    "    ('Title', '@Name'),\n",
    "    ('Author', '@Author'),\n",
    "    ('Year', '@Year'),\n",
    "    ('Reviews', '@Reviews')\n",
    "]\n",
    "\n",
    "p.add_tools(hover)\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(p)\n",
    "save(p)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598d64e08a06bfba065a99698ec5bbec753236817de80e4f3bcf221574aa140c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
